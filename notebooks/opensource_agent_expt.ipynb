{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_clues, generate_abs_path\n",
    "from src.opensource.llama_frontend import chat_pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import pandas as pd\n",
    "from src.opensource.llama_backend import generate_text\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from src.opensource.templates import SYS_FALCON_TEMPLATE, INST_FALCON_TEMPLATE, SYS_LLAMA_TEMPLATE, INST_LLAMA_TEMPLATE\n",
    "from const import STATE_CLUES_NOTES_DICT\n",
    "from dotenv import load_dotenv\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buffer = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name the object on the basis of the above clues from Assam. After your guess, I will let you know if you are correct or not. I do not need to know your reasoning behind the answer. Just tell me the answer and nothing else. If you do not know the answer, say that you do not know the answer. Format your answer in the form of ANSWER: your_answer_here where your_answer_here is your guess. Do not deviate from this answer format.\n"
     ]
    }
   ],
   "source": [
    "inst_template = PromptTemplate.from_template(INST_FALCON_TEMPLATE)\n",
    "inst_template = inst_template.format(state='Assam')\n",
    "print(inst_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.utils import state_clue_notes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/t-sahuja/cultural_artifacts/clues/assam/artifacts_clues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "arti = df.iloc[8]['artifact']\n",
    "clues = df.iloc[8]['clues'].strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The book that studies the life of a very important religious guru.', 'It is not a book used for chanting']\n",
      "Chitra-Bhagavata\n"
     ]
    }
   ],
   "source": [
    "print(clues)\n",
    "print(arti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_template = PromptTemplate.from_template(INST_FALCON_TEMPLATE)\n",
    "inst_template = inst_template.format(state='Assam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\"\n",
    "for j, clue in enumerate(clues):\n",
    "    output += f\"CLUE-{j+1}: {clue.strip()}\\n\"\n",
    "fin_clues = output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLUE-1: The book that studies the life of a very important religious guru.\\nCLUE-2: It is not a book used for chanting'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707c539bdf354af79fb64f35b414cda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text(\"tiiuae/falcon-7b-instruct\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    " llm = HuggingFaceHub(\n",
    "        huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
    "        repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "        model_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 500, \"do_sample\":False},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buffer = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " agent = chat_pipeline(\n",
    "            clue_list=fin_clues,\n",
    "            prompt_text=SYS_FALCON_TEMPLATE,\n",
    "            conversation_buffer=conversation_buffer,\n",
    "            llm=llm,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = agent.predict(human_input=inst_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The object you are thinking of is a <b>Tulkus</b>. It is a Tibetan Buddhist religious figure that is believed to have attained enlightenment and is considered to be a manifestation of a deity. It is not a book used for chanting.\n"
     ]
    }
   ],
   "source": [
    "print(gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferWindowMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Name the object on the basis of the above clues from Assam. After your guess, I will let you know if you are correct or not. I do not need to know your reasoning behind the answer. Just tell me the answer and nothing else. If you do not know the answer, say that you do not know the answer. Format your answer in the form of ANSWER: your_answer_here where your_answer_here is your guess. Do not deviate from this answer format.', additional_kwargs={}, example=False), AIMessage(content=' The object you are thinking of is a <b>Tulkus</b>. It is a Tibetan Buddhist religious figure that is believed to have attained enlightenment and is considered to be a manifestation of a deity. It is not a book used for chanting.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='chat_history', k=2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess2 = agent.predict(human_input=\"Your first guess is not correct. While making your second guess, please stick to the format as ANSWER: your_answer_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The object you are thinking of is a <b>Tulkus</b>. It is a Tibetan Buddhist religious figure that is believed to have attained enlightenment and is considered to be a manifestation of a deity. It is not a book used for chanting.\\nUser '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(columns=['guess1', 'guess2', 'ground_truth', 'clues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(df, df_eval):\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"artifact--{i}---\")\n",
    "        clues = row[\"clues\"].strip().split(\"\\n\")\n",
    "        artifact = row[\"artifact\"].lower().strip()\n",
    "        output = \"\"\n",
    "        for j, clue in enumerate(clues):\n",
    "            output += f\"CLUE-{j+1}: {clue.strip()}\\n\"\n",
    "            fin_clues = output.strip()\n",
    "\n",
    "        agent = chat_pipeline(\n",
    "            clue_list=fin_clues,\n",
    "            prompt_text=SYS_FALCON_TEMPLATE,\n",
    "            conversation_buffer=conversation_buffer,\n",
    "            model_id=\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "        )\n",
    "        guess1 = agent.predict(human_input=inst_template)\n",
    "        print(guess1, \"chek\")\n",
    "        guess1 = guess1.split(\":\")[1].strip().lower() if len(guess1.split(\":\")) > 1 else guess1.strip()\n",
    "        if artifact in guess1:\n",
    "            df_eval = df_eval.append(\n",
    "                {\n",
    "                    \"guess1\": guess1,\n",
    "                    \"guess2\": \"NA\",\n",
    "                    \"ground_truth\": artifact,\n",
    "                    \"clues\": row[\"clues\"].strip(),\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            conversation_buffer.clear()\n",
    "            continue\n",
    "        else:\n",
    "            guess2 = agent.predict(human_input=\"Your first guess is not correct. While making your second guess, please stick to the format as ANSWER: your_answer_here\")\n",
    "            guess2 = guess2.split(\":\")[1].strip().lower() if len(guess2.split(':')) > 1 else guess2.strip()\n",
    "            df_eval = df_eval.append(\n",
    "                {\n",
    "                    \"guess1\": guess1,\n",
    "                    \"guess2\": guess2,\n",
    "                    \"ground_truth\": artifact,\n",
    "                    \"clues\": row[\"clues\"].strip(),\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        conversation_buffer.clear()\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = get_outputs(df, df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = generate_abs_path('results/opensource/llama')\n",
    "os.path.exists(gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('jharkhand_evals_with_state_name.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_results(STATE_CLUES_NOTES_DICT, output_dir):\n",
    "    for key, val in STATE_CLUES_NOTES_DICT.items():\n",
    "        curr_path = os.path.join(output_dir,key)\n",
    "        if not os.path.exists(curr_path):\n",
    "            os.makedirs(curr_path)\n",
    "        \n",
    "        clue_path = val[0]\n",
    "        notes_path = val[1] if len(val) > 1 else None\n",
    "        \n",
    "#         print(\"Getting results for {key}ate\")\n",
    "        print(f\"getting results for {key} state\")\n",
    "        conversation_buffer.clear()\n",
    "        df_clues_eval = pd.DataFrame(columns=['guess1', 'guess2', 'ground_truth', 'clues'])\n",
    "        df_clues = pd.read_csv(clue_path)\n",
    "        \n",
    "        \n",
    "        print(f\"Running clues eval for {key} state\")\n",
    "        clues_result_path = os.path.join(curr_path,'eval_clues.csv')\n",
    "        if not os.path.exists(clues_result_path):\n",
    "            df_clues_eval = get_outputs(df_clues, df_clues_eval)\n",
    "            df_clues_eval.to_csv(clues_result_path, index=False)\n",
    "        else:\n",
    "            print(f\"Clue eval results already exist for {key} state\")\n",
    "        \n",
    "        if notes_path:\n",
    "            conversation_buffer.clear()\n",
    "            df_notes = pd.read_csv(notes_path)\n",
    "            df_notes_eval = pd.DataFrame(columns=['guess1', 'guess2', 'ground_truth', 'clues'])\n",
    "            notes_result_path = os.path.join(curr_path,'eval_notes.csv')\n",
    "            if not os.path.exists(notes_result_path):\n",
    "                print(f\"Running notes eval for {key} state\")\n",
    "                df_notes_eval = get_outputs(df_notes, df_clues_eval)\n",
    "                df_notes_eval.to_csv(notes_result_path, index=False)\n",
    "            else:\n",
    "                print(f\"Notes eval results already exist for {key} state\")\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_results(STATE_CLUES_NOTES_DICT, '/home/t-sahuja/cultural_artifacts/results/opensource/llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:culture] *",
   "language": "python",
   "name": "conda-env-culture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
